{"meta":{"title":"SeoWon💕, SeoHun💕, EunA💕","subtitle":null,"description":null,"author":"고재성","url":"http://kjs850.github.io"},"pages":[],"posts":[{"title":"git squash","slug":"git-squash","date":"2018-12-02T11:45:57.915Z","updated":"2018-12-02T11:45:57.915Z","comments":true,"path":"2018/12/02/git-squash/","link":"","permalink":"http://kjs850.github.io/2018/12/02/git-squash/","excerpt":"","text":"date: 2018-12-02 20:26:59 준비1234567891011121314151617181920212223242526272829 jake.ko  ~/Dev  cd git-study jake.ko  ~/Dev/git-study  ll jake.ko  ~/Dev/git-study  git initInitialized empty Git repository in /Users/jake.ko/Dev/git-study/.git/ jake.ko  ~/Dev/git-study   master  ll jake.ko  ~/Dev/git-study   master  ll -alrtotal 0drwxr-xr-x 10 jake.ko staff 320B 12 2 20:21 .gitdrwxr-xr-x 22 jake.ko staff 704B 12 2 20:16 ..drwxr-xr-x 3 jake.ko staff 96B 12 2 20:17 . jake.ko  ~/Dev/git-study   master  echo first &gt; first.txt jake.ko  ~/Dev/git-study   master  git add . jake.ko  ~/Dev/git-study   master ✚  git commit -m \"A\"[master (root-commit) 20611a0] A 1 file changed, 1 insertion(+) create mode 100644 first.txt jake.ko  ~/Dev/git-study   master  echo second &gt; second.txt jake.ko  ~/Dev/git-study   master  git add . jake.ko  ~/Dev/git-study   master ✚  git commit -m \"B\"[master 381d030] B 1 file changed, 1 insertion(+) create mode 100644 second.txt jake.ko  ~/Dev/git-study   master  echo third &gt; third.txt jake.ko  ~/Dev/git-study   master  git add . jake.ko  ~/Dev/git-study   master ✚  git commit -m \"C\"[master 52bfd68] C 1 file changed, 1 insertion(+) create mode 100644 third.txt jake.ko  ~/Dev/git-study   master  git log1234567891011121314151617commit 52bfd68372367dcbb2b4c1c5bb1ddb1086f02119 (HEAD -&gt; master)Author: jake.ko &lt;jake.ko@kakaocorp.com&gt;Date: Sun Dec 2 20:23:30 2018 +0900 Ccommit 381d030dafd61153bb3754c260b7216a9b5c47e9Author: jake.ko &lt;jake.ko@kakaocorp.com&gt;Date: Sun Dec 2 20:22:52 2018 +0900 Bcommit 20611a043cea164d8d3e5f5d73b3949974a5fe50Author: jake.ko &lt;jake.ko@kakaocorp.com&gt;Date: Sun Dec 2 20:22:19 2018 +0900 A git rebase -i HEAD~2123456789101112131415161718192021pick 381d030 Bpick 52bfd68 C# Rebase 20611a0..52bfd68 onto 20611a0 (2 commands)## Commands:# p, pick = use commit# r, reword = use commit, but edit the commit message# e, edit = use commit, but stop for amending# s, squash = use commit, but meld into previous commit# f, fixup = like \"squash\", but discard this commit's log message# x, exec = run command (the rest of the line) using shell# d, drop = remove commit## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.## Note that empty commits are commented out 바꾸고 저장123pick 381d030 Bsquash 52bfd68 C 아래와 같은 수정이 가능 , 디폴트 그대로 저장 (.git/COMMIT_EDITMSG)12345678910111213141516171819202122232425# This is a combination of 2 commits.# This is the 1st commit message:B# This is the commit message #2:C# Please enter the commit message for your changes. Lines starting# with '#' will be ignored, and an empty message aborts the commit.## Date: Sun Dec 2 20:22:52 2018 +0900## interactive rebase in progress; onto 20611a0# Last commands done (2 commands done):# pick 381d030 B# squash 52bfd68 C# No commands remaining.# You are currently rebasing branch 'master' on '20611a0'.## Changes to be committed:# new file: second.txt# new file: third.txt# 결과1234567jake.ko  ~/Dev/git-study   master  git rebase -i HEAD~2[detached HEAD 6899e57] B Date: Sun Dec 2 20:22:52 2018 +0900 2 files changed, 2 insertions(+) create mode 100644 second.txt create mode 100644 third.txtSuccessfully rebased and updated refs/heads/master git log123456789101112131415'git help -a' and 'git help -g' list available subcommands and somecommit 6899e57839eb085a8f0188572aec93ab1b8db74b (HEAD -&gt; master)Author: jake.ko &lt;jake.ko@kakaocorp.com&gt;Date: Sun Dec 2 20:22:52 2018 +0900 B Ccommit 20611a043cea164d8d3e5f5d73b3949974a5fe50Author: jake.ko &lt;jake.ko@kakaocorp.com&gt;Date: Sun Dec 2 20:22:19 2018 +0900 A(END) tags: [git]","categories":[],"tags":[]},{"title":"Kafka Consumer offset reset - java code","slug":"Kafka-Consumer-offset-reset-java-code","date":"2018-11-28T09:21:53.081Z","updated":"2018-11-28T09:21:53.081Z","comments":true,"path":"2018/11/28/Kafka-Consumer-offset-reset-java-code/","link":"","permalink":"http://kjs850.github.io/2018/11/28/Kafka-Consumer-offset-reset-java-code/","excerpt":"","text":"date: 2018-11-28 18:09:53 java로 특정 시간의 offset 부터 가져오기123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package com.example.kafkaoffsettest;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.clients.consumer.OffsetAndTimestamp;import org.apache.kafka.common.TopicPartition;import org.joda.time.DateTime;import java.time.Instant;import java.util.*;import static com.example.kafkaoffsettest.KafkaConsumerUtil.TOPIC;import static com.example.kafkaoffsettest.KafkaConsumerUtil.createConsumer;import static java.time.temporal.ChronoUnit.DAYS;import static java.time.temporal.ChronoUnit.MINUTES;public class KafkaConsumerFromTime &#123; public static void main(String[] args) &#123; KafkaConsumer&lt;String, String&gt; consumer = createConsumer(); consumer.subscribe(Arrays.asList(TOPIC)); boolean flag = true; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); if (flag) &#123; Set&lt;TopicPartition&gt; assignments = consumer.assignment(); Map&lt;TopicPartition, Long&gt; query = new HashMap&lt;&gt;(); for (TopicPartition topicPartition : assignments) &#123; query.put( topicPartition, Instant.now().minus(1, DAYS).toEpochMilli()); &#125; Map&lt;TopicPartition, OffsetAndTimestamp&gt; result = consumer.offsetsForTimes(query); result.entrySet() .stream() .forEach(entry -&gt; consumer.seek( entry.getKey(), Optional.ofNullable(entry.getValue()) .map(OffsetAndTimestamp::offset) .orElse(new Long(0)))); flag = false; &#125; for (ConsumerRecord&lt;String, String&gt; record : records) &#123; long timestamp = record.timestamp(); DateTime dateTime = new DateTime(timestamp); System.out.printf(\"dateTime = %s, offset = %d, key = %s, value = %s%n\", dateTime, record.offset(), record.key(), record.value()); &#125; &#125; &#125;&#125; 결과1234567&gt; date Wed Nov 28 18:20:25 KST 2018//실행결과dateTime = 2018-11-28T17:43:58.201+09:00, offset = 26, key = null, value = foo1dateTime = 2018-11-28T17:43:58.208+09:00, offset = 27, key = null, value = foo2dateTime = 2018-11-28T17:43:58.208+09:00, offset = 28, key = null, value = foo3 tags: [kafka, offset, java]","categories":[],"tags":[]},{"title":"Kafka Consumer offset reset 하기","slug":"Kafka-Consumer-offset-reset-하기","date":"2018-11-25T14:56:30.189Z","updated":"2018-11-25T14:56:30.190Z","comments":true,"path":"2018/11/25/Kafka-Consumer-offset-reset-하기/","link":"","permalink":"http://kjs850.github.io/2018/11/25/Kafka-Consumer-offset-reset-하기/","excerpt":"","text":"date: 2018-11-22 16:25:22 그룹 지정하여 토픽 수신123456789101112 ✘ jake.ko@jakekoui-MacBook-Pro  ~  kafka-console-consumer --bootstrap-server localhost:9092 --topic test --group testGroup --from-beginningaaabbbccc``` ## 컨슈머 그룹별 offset 상태 확인```bash✘ jake.ko@jakekoui-MacBook-Pro  ~  kafka-consumer-groups --bootstrap-server localhost:9092 --group testGroup --describeTOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-IDtest 0 3 3 0 consumer-1-39ea110a-7f65-4ec3-8a9d-22c82d8be469 /172.26.113.148 consumer-1 TOPIC: 토픽 이름 PARTITION: consumer group 내의 각 consumer가 할당된 파티션 번호 CURRENT-OFFSET: 현재 consumer group의 consumer가 각 파티션에서 마지막으로 offset을 commit한 값 LOG-END-OFFSET: producer쪽에서 마지막으로 생성한 레코드의 offset LAG: LOG-END-OFFSET에서 CURRENT-OFFSET를 뺀 값. 컨슈머 그룹의 offset reset1kafka-consumer-groups --bootstrap-server localhost:9092 --group testGroup --topic test --reset-offsets --to-earliest --execute –topic 대신 –all-topics를 지정하면 모든 토픽에 대해서 실행이 가능하다. –execute 옵션을 제거하고, –dry-run옵션으로 실행하면 실제 반영되지 않고 어떻게 변할지 결과만 출력하는 dry run이 가능하다. 1234 jake.ko@jakekoui-MacBook-Pro  ~  kafka-consumer-groups --bootstrap-server localhost:9092 --group testGroup --topic test --reset-offsets --to-earliest --dry-runTOPIC PARTITION NEW-OFFSETtest 0 0 주의사항: 해당 그룹의 컨슈머를 멈추고 리셋해야 한다.(consumer group이 실행중인 상태에 offset reset을 진행하는 경우 reset은 실패한다.) 12345 jake.ko@jakekoui-MacBook-Pro  ~  kafka-consumer-groups --bootstrap-server localhost:9092 --group testGroup --topic test --reset-offsets --to-earliest --dry-runError: Assignments can only be reset if the group 'testGroup' is inactive, but the current state is Stable.TOPIC PARTITION NEW-OFFSET jake.ko@jakekoui-MacBook-Pro  ~  샘플기존의 프로듀서가 지속적으로 메시지 생산1234567jake.ko@jakekoui-MacBook-Pro  ~  kafka-console-producer --broker-list localhost:9092 --topic test&gt;aaa&gt;bbb&gt;ccc&gt;ddd&gt;eee&gt; 중간부터 컨슘123✘ jake.ko@jakekoui-MacBook-Pro  ~  kafka-console-consumer --bootstrap-server localhost:9092 --topic test --group testGroupdddeee 리셋 후 처음부터 컨슘하도록 변경12345jake.ko@jakekoui-MacBook-Pro  ~  kafka-consumer-groups --bootstrap-server localhost:9092 --group testGroup --topic test --reset-offsets --to-earliest --executeTOPIC PARTITION NEW-OFFSETtest 0 0 jake.ko@jakekoui-MacBook-Pro  ~  컨슘 재개 후 결과 (offset 리셋되어, 처음부터 컨슘한다.)123456 ✘ jake.ko@jakekoui-MacBook-Pro  ~  kafka-console-consumer --bootstrap-server localhost:9092 --topic test --group testGroupaaabbbcccdddeee 오프셋의 위치를 재설정하기 위한 아래와같은 상세 옵션들이 있다. –shift-by &lt;Long: number-of-offsets&gt; 형식 (+/- 모두 가능) –to-offset &lt;Long: offset&gt; –to-current –by-duration &lt;String: duration&gt; : 형식 ‘PnDTnHnMnS’ –to-datetime &lt;String: datetime&gt; : 형식 ‘YYYY-MM-DDTHH:mm:SS.sss’ –to-latest –to-earliest –to-datetime의 경우 kafka에서 데이터를 읽어서 다른곳에 저장하는 중에 데이터 유실 또는 중복 write 등이 발생한 경우에 날짜 단위로 데이터를 다시 불러와서 재처리하고 싶은 경우 매우 유용하다. CLI가 아닌 Java 코드로 offset reset하기Kafka의 경우 사용 형태에 따라 Consumer API와 Stream API 두가지를 제공한다. Consumer API 개별 이벤트 단위의 low level 처리가 필요한 경우 datetime, offset 을 지정해서 원하는 대로 reset 가능 Stream API Stream processing이 필요한 경우 offset reset 기능 없음 (위에서 언급한 CLI tool을 사용해야함) Consumer API를 사용할때 Java코드 레벨에서 programmatical하게 offset을 리셋하는 방법은 다음과 같다.먼저 KafkaConsumer가 생성한 후에1KafkaConsumer&lt;Object, Object&gt; consumer = new KafkaConsumer&lt;&gt;(properties, keyDeser, valueDeser); consumer loop에 진입하여 consumer.poll()을 부르기 전에, 생성된 consumer 객체에 대해 offset을 변경하는 다음 함수들을 호출하여 offset을 원하는 대로 설정할 수 있다. seekToBeginning: earliest로 reset seekToEnd: latest로 reset seek : 지정 offset으로 reset offsetsForTimes: datetime 기준으로 reset reference https://www.letmecompile.com/kafka-consumer-offset-reset/ https://gist.github.com/marwei/cd40657c481f94ebe273ecc16601674b http://blog.sysco.no/integration/kafka-rewind-consumers-offset/ tags: kafka, offset","categories":[],"tags":[]},{"title":"kafka 설치 및 구동","slug":"kafka-설치-및-구동","date":"2018-11-22T07:28:12.775Z","updated":"2018-11-22T07:28:12.775Z","comments":true,"path":"2018/11/22/kafka-설치-및-구동/","link":"","permalink":"http://kjs850.github.io/2018/11/22/kafka-설치-및-구동/","excerpt":"","text":"date: 2018-11-22 14:35:57 설치1234567891011121314151617181920&gt; brew install kafka..... 생략... brew services start kafkaOr, if you don't want/need a background service you can just run: zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties &amp; kafka-server-start /usr/local/etc/kafka/server.properties==&gt; Summary🍺 /usr/local/Cellar/kafka/2.0.0: 160 files, 46.8MB==&gt; Caveats==&gt; zookeeperTo have launchd start zookeeper now and restart at login: brew services start zookeeperOr, if you don't want/need a background service you can just run: zkServer start==&gt; kafkaTo have launchd start kafka now and restart at login: brew services start kafkaOr, if you don't want/need a background service you can just run: zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties &amp; kafka-server-start /usr/local/etc/kafka/server.properties 주키퍼 구동 12345678jake.ko@jakekoui-MacBook-Pro  ~  zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties[2018-11-22 14:39:11,550] INFO Reading configuration from: /usr/local/etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)[2018-11-22 14:39:11,553] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)[2018-11-22 14:39:11,553] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)[2018-11-22 14:39:11,555] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)[2018-11-22 14:39:11,555] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)[2018-11-22 14:39:11,571] INFO Reading configuration from: /usr/local/etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)생략... 카프카 구동 1234567891011jake.ko@jakekoui-MacBook-Pro  ~  kafka-server-start /usr/local/etc/kafka/server.properties[2018-11-22 14:39:25,676] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)[2018-11-22 14:39:26,191] INFO starting (kafka.server.KafkaServer)[2018-11-22 14:39:26,192] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)[2018-11-22 14:39:26,208] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)[2018-11-22 14:39:26,215] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)[2018-11-22 14:39:26,215] INFO Client environment:host.name=172.26.113.148 (org.apache.zookeeper.ZooKeeper)[2018-11-22 14:39:26,215] INFO Client environment:java.version=1.8.0_161 (org.apache.zookeeper.ZooKeeper)[2018-11-22 14:39:26,215] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)[2018-11-22 14:39:26,215] INFO Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home/jre (org.apache.zookeeper.ZooKeeper)생략... 토픽 생성12jake.ko@jakekoui-MacBook-Pro  ~  kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic testCreated topic \"test\". 토픽 보내기(produce)12345 jake.ko@jakekoui-MacBook-Pro  ~  kafka-console-producer --broker-list localhost:9092 --topic test&gt;aaa&gt;bbb&gt;ccc&gt; 토픽 수신(consume)1234 jake.ko@jakekoui-MacBook-Pro  ~  kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginningaaabbbccc reference https://medium.com/@Ankitthakur/apache-kafka-installation-on-mac-using-homebrew-a367cdefd273 tags: kafka, mac, install","categories":[],"tags":[]},{"title":"kafka streams는 무엇인가?","slug":"kafka-streams는-무엇인가","date":"2018-11-22T04:19:54.220Z","updated":"2018-11-22T04:19:54.220Z","comments":true,"path":"2018/11/22/kafka-streams는-무엇인가/","link":"","permalink":"http://kjs850.github.io/2018/11/22/kafka-streams는-무엇인가/","excerpt":"","text":"date: 2018-11-22 10:48:50 소개 카프카 스트림즈는 카프카에 저장된 데이터를 처리하고 분석하기 위해 개발된 클라이언트 라이브러리다. 카프카 스트림즈는 이벤트 처리 시간과 처리 시간을 분리해서 다루고 다양한 시간 간격 옵션을 지원하기에 실시간 분석을 간단하면서도 효율적으로 진행할 수 있다. 특징 간단하고 가벼운 클라이언트 라이브러리이기 때문에 기존 애플리케이션이나 자바 애플리케이션에서 쉽게 사용할수 있습니다. 시스템이나 카프카에 대한 의존성이 없습니다. 이중화된 로컬 상태 저장소를 지원합니다. 카프카 브로커나 클라이언트에 장애가 생기더라도 스트림에 대해서 1번만 처리가 되도록 보장합니다. 밀리초 단위의 처리 지연을 보장하기 위해 한번에 한 레코드만 처리합니다. 간단하게 스트림 처리 프로그램을 만들 수 있도록 고수준의 스트림 DSL(Domain Specific Language)을 지원하고, 저수준의 프로세싱 API도 제공합니다. 스트림 만들기 예제1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package test.kafka; import org.apache.kafka.common.serialization.Serdes;import org.apache.kafka.streams.KafkaStreams;import org.apache.kafka.streams.StreamsBuilder;import org.apache.kafka.streams.StreamsConfig;import org.apache.kafka.streams.Topology; import java.util.Properties;import java.util.concurrent.CountDownLatch; public class Pipe &#123; public static void main(String[] args) throws Exception &#123; Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"jake-streams-pipe\"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\"); props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass()); props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass()); final StreamsBuilder builder = new StreamsBuilder(); builder.stream(\"jake-stream-in\").to(\"jake-stream-out\"); final Topology topology = builder.build(); System.out.println(topology.describe()); final KafkaStreams streams = new KafkaStreams(topology, props); final CountDownLatch latch = new CountDownLatch(1); // attach shutdown handler to catch control-c Runtime.getRuntime().addShutdownHook(new Thread(\"streams-shutdown-hook\") &#123; @Override public void run() &#123; streams.close(); latch.countDown(); &#125; &#125;); try &#123; streams.start(); latch.await(); &#125; catch (Throwable e) &#123; System.exit(1); &#125; System.exit(0); &#125; &#125; 출력 결과아래와 같은 토폴로지가 만들어진다. 하나의 토픽을 그대로 복사한 토픽이 생겼다. ( jake-stream-in 의 토픽내의 메시지를 실시간으로 복사하는 jake-stream-out 토픽이 생겼다) 응용jake-stream-out이란 토픽에 실시간으로 분석한 결과를 넣을 수 있다. (ex. 단어 split , 또는 단어 빈도 세기) 참고 카프카 console-producer console-consumer는 그냥 콘솔로 메시지 생성, 소비한다. 카프카 설치 파일의 bin에 위치해 있음. (http://kafka.apache.org/downloads.html)","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2018-11-21T08:01:00.716Z","updated":"2018-11-21T08:01:00.716Z","comments":true,"path":"2018/11/21/hello-world/","link":"","permalink":"http://kjs850.github.io/2018/11/21/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Starthttps://hyunseob.github.io/2016/02/23/start-hexo/ Create a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}